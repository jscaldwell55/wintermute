"""
Core memory system implementing the direct-to-Pinecone approach.
"""

from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from core.memory.models import Memory, MemoryType
from utils.vector_operations import VectorOperations
from utils.pinecone_service import PineconeService
from core.prompts.prompt_templates import (
    RESEARCH_CONTEXT_TEMPLATE,
    PERSONAL_ASSISTANT_TEMPLATE,
    format_prompt
)
import logging
import numpy as np
import uuid
import asyncio
import math

logger = logging.getLogger(__name__)

class MemorySystem:
    """Core memory system using Pinecone for storage."""

    def __init__(self, pinecone_api_key: str, pinecone_environment: str, vector_operations: VectorOperations):
        self.pinecone_service = PineconeService(pinecone_api_key, pinecone_environment)
        self.vector_operations = vector_operations
        self.retention_window = timedelta(days=7)

    async def add_memory(self, content: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        """
        Adds a memory to the system, storing it directly in Pinecone.

        Args:
            content: The content of the memory.
            metadata: Optional metadata for the memory.

        Returns:
            The unique ID of the added memory.
        """
        try:
            if not content.strip():
                raise ValueError("Memory content cannot be empty.")
            if metadata is not None and not isinstance(metadata, dict):
                raise ValueError("Metadata must be a dictionary.")

            # Generate semantic vector
            semantic_vector = await self.vector_operations.create_semantic_vector(content)
            if len(semantic_vector) != 1536:
                raise ValueError("Generated semantic vector has incorrect dimensionality.")

            # Generate a unique memory ID
            memory_id = f"mem_{uuid.uuid4()}"

            # Prepare metadata
            metadata = metadata or {}
            current_time = datetime.now().isoformat()

            # Update metadata with required fields
            full_metadata = {
                "content": content,
                "created_at": current_time,
                "memory_type": "TEMPORAL",
                **metadata  # Include any additional metadata passed in
            }

            # Create memory object
            memory = Memory(
                id=memory_id,
                memory_type="TEMPORAL",
                content=content,
                semantic_vector=semantic_vector.tolist(),  # Use semantic vector
                metadata=full_metadata,
                created_at=current_time
            )

            # Log and upsert
            logger.info(f"Upserting memory with ID: {memory_id} and metadata: {full_metadata}")

            # Upsert to Pinecone
            await self.pinecone_service.upsert_memory(
                memory_id=memory.id,
                vector=memory.semantic_vector,  # Already a list because of Memory object
                metadata=memory.metadata
            )

            return memory.id

        except Exception as e:
            logger.error(f"Error adding memory: {e}")
            raise RuntimeError(f"Error adding memory: {e}")
    
    async def add_interaction_memory(self, user_query: str, gpt_response: str):
        """
        Stores a user query and its corresponding GPT response as an interaction memory.

        Args:
            user_query: The user's query.
            gpt_response: The response generated by GPT.
        """
        try:
            timestamp = datetime.now().isoformat()

            # Memory for the user query
            query_metadata = {
                "memory_type": "INTERACTION",
                "role": "user",
                "timestamp": timestamp
            }
            await self.add_memory(content=user_query, metadata=query_metadata)

            # Memory for the GPT response
            response_metadata = {
                "memory_type": "INTERACTION",
                "role": "assistant",
                "timestamp": timestamp
            }
            await self.add_memory(content=gpt_response, metadata=response_metadata)

            logger.info("Interaction memory successfully stored.")

        except Exception as e:
            logger.error(f"Error adding interaction memory: {e}")
            raise RuntimeError(f"Error adding interaction memory: {e}")

    async def consolidate_memories(self):
        """
        Consolidates old memories into a single grounding memory.

        Fetches memories older than the retention period, merges their semantic vectors and metadata,
        and replaces the old records with a compressed memory.
        """
        try:
            old_memories = await self._get_old_memories(days=30)
            if not old_memories:
                logger.info("No old memories to consolidate.")
                return

            # Ensure valid semantic vectors are used
            semantic_vectors = [np.array(mem.semantic_vector) for mem, _ in old_memories if mem.semantic_vector is not None]
            if not semantic_vectors:
                logger.warning("No valid semantic vectors found for consolidation.")
                return

            # Merge semantic vectors
            merged_vector = self.vector_operations.average_vectors(semantic_vectors)
            if isinstance(merged_vector, np.ndarray):
                merged_vector = merged_vector.tolist()  # Convert NumPy array to list if necessary

            # Merge metadata
            merged_metadata = self.merge_metadata([mem for mem, _ in old_memories])

            # Create a compressed memory
            compressed_memory = Memory(
                id=f"compressed_{uuid.uuid4()}",
                content="Summarized content",  # You might want to generate more meaningful content here
                semantic_vector=merged_vector,
                metadata=merged_metadata,
                created_at=datetime.now().isoformat(),
                memory_type="GROUNDING",
            )

            # Store the compressed memory
            await self.pinecone_service.upsert_memory(
                compressed_memory.id,
                compressed_memory.semantic_vector,
                compressed_memory.metadata
            )

            # Delete old memories
            for mem, _ in old_memories:
                await self.pinecone_service.delete_memory(mem.id)
                logger.info(f"Deleted old memory: {mem.id}")

            logger.info(f"Consolidated {len(old_memories)} memories into ID: {compressed_memory.id}")

        except Exception as e:
            logger.error(f"Error during memory consolidation: {e}")
            raise

    async def query_memory(self, query_vector: np.ndarray, query_type: MemoryType, k: int = 5) -> List[Tuple[Memory, float]]:
        """Queries the memory system."""
        try:
            if len(query_vector) != 1536:
                raise ValueError("Query vector dimensionality does not match the index configuration.")

            filters = self.build_metadata_filter(query_type)
            response = await self.pinecone_service.query_memory(query_vector.tolist(), top_k=k, filters=filters)

            # Ensure we access the 'matches' key from the Pinecone response
            results = response.get("matches", []) if isinstance(response, dict) else response

            # Convert results into Memory objects with scores
            memories_with_scores = [
                (self._create_memory_from_result(result), result["score"]) for result in results
        ]
            return self.rank_and_filter_results(memories_with_scores, k)

        except ValueError as e:
            logger.error(f"Error querying memory: {e}")
            return []

        except Exception as e:
            logger.error(f"Error querying memory: {e}")
            raise

    async def search_memories(self, query_vector: np.ndarray, top_k: int = 5) -> str:
        """
        Searches for relevant memories and formats them for prompt templates.

        Args:
            query_vector (np.ndarray): The query vector.
            top_k (int): The number of top memories to retrieve.

        Returns:
            str: The formatted string containing the retrieved context.
        """
        results = await self.query_memory(
            query_vector=query_vector,
            query_type=MemoryType.TEMPORAL,
            k=top_k
    )

        # Process results to apply decay and combine scores
        final_results = []
        for memory, similarity_score in results:
            created_at = memory.metadata.get("created_at")
            if created_at:
                decay_weight = self.calculate_decay_weight(created_at)
                final_score = similarity_score * decay_weight
            else:
                final_score = 0.0
            final_results.append((memory, final_score))

        # Rank results by final score
        ranked_results = sorted(final_results, key=lambda x: x[1], reverse=True)[:top_k]

        # Format the memories for the prompt template
        retrieved_context = "\n".join([f"- {memory.content}" for memory, _ in ranked_results])

        return retrieved_context

    async def generate_prompt(self, query: str, template_type: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        """
        Retrieves memories, formats them into a prompt template, and returns the full prompt.

        Args:
            query (str): The user's query.
            template_type (str): Type of prompt template (e.g., 'RESEARCH_CONTEXT_TEMPLATE').
            metadata (dict): Additional metadata required by the template.

        Returns:
            str: A formatted prompt.
        """
        query_vector = await self.vector_operations.create_semantic_vector(query)
        retrieved_context = await self.search_memories(query_vector)

        template = {
            "research_context": RESEARCH_CONTEXT_TEMPLATE,
            "personal_assistant": PERSONAL_ASSISTANT_TEMPLATE,
        }.get(template_type)

        if not template:
            raise ValueError(f"Unknown template type: {template_type}")

        prompt = format_prompt(
            template,
            retrieved_context=retrieved_context,
            user_query=query,
            **(metadata or {})
        )
        return prompt

    def calculate_decay_weight(self, created_at: str, decay_rate: float = 0.1) -> float:
        """
        Calculate a decay weight based on memory age.

        Args:
            created_at (str): The creation timestamp of the memory.
            decay_rate (float): The rate of decay. Higher values decay faster.

        Returns:
            float: The decay weight.
        """
        try:
            memory_date = datetime.fromisoformat(created_at)
            days_elapsed = (datetime.now() - memory_date).days
            return math.exp(-decay_rate * days_elapsed)
        except Exception as e:
            logger.error(f"Error calculating decay weight: {e}")
            return 0.0  # Return zero weight if there's an error
    
    def build_metadata_filter(self, query_type: MemoryType) -> Dict[str, Any]:
        """
        Builds a metadata filter for Pinecone queries based on memory type.

        Args:
            query_type: The type of memory to prioritize (TEMPORAL or GROUNDING).

        Returns:
            Dict[str, Any]: Metadata filter for Pinecone.
        """
        if query_type == MemoryType.TEMPORAL:
            # Temporal memories: Filter for memories created in the last 7 days
            cutoff_date = (datetime.now() - timedelta(days=7)).isoformat()
            return {"created_at": {"$gte": cutoff_date}}

        # Default: No filtering for grounding memories
        return {}

    def rank_and_filter_results(self, results: List[Tuple[Memory, float]], top_k: int) -> List[Tuple[Memory, float]]:
        """Ranks and filters memory query results."""
        return sorted(results, key=lambda x: x[1], reverse=True)[:top_k]

    async def _get_old_memories(self, days: int) -> List[Tuple[Memory, float]]:
        """Retrieves memories older than a specified number of days."""
        cutoff_time = datetime.now() - timedelta(days=days)
        try:
            all_memories = await self.pinecone_service.get_all_memories_with_metadata()
            return [(self._create_memory_from_result(mem), 0) for mem in all_memories if datetime.fromisoformat(mem["metadata"]["created_at"]) < cutoff_time]
        except Exception as e:
            logger.error(f"Error fetching old memories: {e}")
            return []

    def merge_metadata(self, memories: List[Memory]) -> Dict:
        """Merges metadata from multiple memories."""
        merged_metadata = {"content": " ".join([mem.content for mem in memories])}
        for memory in memories:
            for key, value in memory.metadata.items():
                if key in merged_metadata:
                    merged_metadata[key] += "; " + value if isinstance(value, str) else merged_metadata[key]
                else:
                    merged_metadata[key] = value
        return merged_metadata

    def _create_memory_from_result(self, result: Dict[str, Any]) -> Memory:
        """Creates a Memory object from a Pinecone query result."""
        metadata = result.get("metadata", {})
        return Memory(
            id=result.get("id", ""),
            content=metadata.get("content", ""),
            created_at=metadata.get("created_at", datetime.now().isoformat()),
            memory_type=metadata.get("memory_type", MemoryType.TEMPORAL),
            semantic_vector=result.get("values", []),
            metadata=metadata
        )