"""
Core memory system implementing the direct-to-Pinecone approach.
"""

from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from api.core.memory.models import Memory, MemoryType
from api.core.vector_operations import VectorOperations
from api.utils.pinecone_service import PineconeService
from api.utils.prompt_templates import (
    MASTER_TEMPLATE,
    format_prompt,
)
import logging
import numpy as np
import uuid
import asyncio
import math
import project_root.backend.api.utils.config as config
from api.utils.task_queue import task_queue

logger = logging.getLogger(__name__)

class MemorySystem:
    """Core memory system using Pinecone for storage."""

    def __init__(
        self, pinecone_service: PineconeService, vector_operations: VectorOperations
    ):
        self.pinecone_service = pinecone_service
        self.vector_operations = vector_operations
        self.retention_window = timedelta(days=7)

    async def add_memory(
        self,
        content: str,
        memory_type: MemoryType,
        metadata: Optional[Dict[str, Any]] = None,
        window_id: Optional[str] = None,
        task_id: Optional[str] = None,
    ) -> str:
        """
        Adds a memory to the system, storing it directly in Pinecone.

        Args:
            content: The content of the memory.
            memory_type: The type of memory (EPISODIC or SEMANTIC).
            metadata: Optional metadata for the memory.
            window_id: Optional window ID for episodic memories.

        Returns:
            The unique ID of the added memory.
        """
        try:
            if not content.strip():
                raise ValueError("Memory content cannot be empty.")
            if metadata is not None and not isinstance(metadata, dict):
                raise ValueError("Metadata must be a dictionary.")

            # Generate semantic vector
            semantic_vector = await self.vector_operations.create_semantic_vector(
                content
            )
            if len(semantic_vector) != 1536:
                raise ValueError(
                    "Generated semantic vector has incorrect dimensionality."
                )

            # Generate a unique memory ID
            memory_id = task_id or f"mem_{uuid.uuid4()}"

            # Prepare metadata
            metadata = metadata or {}
            current_time = datetime.now().isoformat()

            # Update metadata with required fields
            full_metadata = {
                "content": content,
                "created_at": current_time,
                "memory_type": memory_type.value,  # Store as string value
                **(metadata),
            }

            # Add window_id to metadata if provided
            if window_id and memory_type == MemoryType.EPISODIC:
                full_metadata["window_id"] = window_id

            # Create memory object
            memory = Memory(
                id=memory_id,
                memory_type=memory_type,
                content=content,
                semantic_vector=semantic_vector,
                metadata=full_metadata,
                created_at=current_time,
                window_id=window_id,
            )

            # Log and upsert
            logger.info(
                f"Upserting memory with ID: {memory_id} and metadata: {full_metadata}"
            )

            # Upsert to Pinecone
            await self.pinecone_service.upsert_memory(
                memory_id=memory.id,
                vector=memory.semantic_vector,
                metadata=memory.metadata,
            )

            return memory.id

        except Exception as e:
            logger.error(f"Error adding memory: {e}")
            raise RuntimeError(f"Error adding memory: {e}")

    async def add_interaction_memory(
        self, user_query: str, gpt_response: str, window_id: Optional[str] = None
    ):
        """
        Stores a user query and its corresponding GPT response as an interaction memory (episodic).

        Args:
            user_query: The user's query.
            gpt_response: The response generated by GPT.
            window_id: ID of current context window
        """
        try:
            # Create combined vector for Q/R pair
            combined_vector = await self.vector_operations.create_combined_q_r_vector(
                user_query, gpt_response
            )

            # Generate a unique task ID for the memory
            task_id = f"mem_{uuid.uuid4()}"

            # Enqueue the memory addition task with retries
            task_queue.enqueue(
                self.add_memory,
                content=f"Q: {user_query}\nA: {gpt_response}",
                memory_type=MemoryType.EPISODIC,
                metadata={"interaction": True, "window_id": window_id},
                semantic_vector=combined_vector,
                window_id=window_id,
                task_id=task_id,  # Pass the unique task ID
                retries=config.SUMMARY_RETRIES,
                retry_delay=config.SUMMARY_RETRY_DELAY,
            )

            logger.info(
                f"Interaction memory added to queue for window {window_id} with task ID {task_id}."
            )

        except Exception as e:
            logger.error(f"Error adding interaction memory: {e}")
            raise RuntimeError(f"Error adding interaction memory: {e}")

    async def query_memory(
        self,
        query_vector: List[float],
        query_types: List[MemoryType],
        top_k: int = 5,
        window_id: Optional[str] = None,
    ) -> List[Tuple[Memory, float]]:
        """
        Queries the memory system for memories of specified types.

        Args:
            query_vector: The query vector.
            query_types: A list of MemoryTypes to query.
            top_k: The number of top results to return.
            window_id: Optional window ID to filter episodic memories.

        Returns:
            A list of tuples, where each tuple contains a Memory object and its similarity score.
        """
        try:
            if len(query_vector) != 1536:
                raise ValueError(
                    "Query vector dimensionality does not match the index configuration."
                )

            filters = self.build_metadata_filter(
                query_types=query_types, window_id=window_id
            )
            response = await self.pinecone_service.query_memory(
                query_vector=query_vector, top_k=top_k, filters=filters
            )

            results = (
                response.get("matches", []) if isinstance(response, dict) else response
            )

            memories_with_scores = [
                (self._create_memory_from_result(result), result["score"])
                for result in results
            ]
            return self.rank_and_filter_results(memories_with_scores, top_k)

        except ValueError as e:
            logger.error(f"Error querying memory: {e}")
            return []

        except Exception as e:
            logger.error(f"Error querying memory: {e}")
            raise

    async def batch_embed_and_store(self, memories: List[Dict[str, Any]]):
        """
        Batch embeds and stores memories in Pinecone.

        Args:
            memories: A list of dictionaries, each representing a memory with 'content' and optional 'metadata'.
        """
        try:
            # Extract the IDs and content from each memory for embedding and upsertion
            memory_ids = [mem["id"] for mem in memories]
            memory_contents = [mem["content"] for mem in memories]
            memory_metadatas = [mem.get("metadata", {}) for mem in memories]

            # Create semantic vectors for all memories in the batch
            semantic_vectors = await asyncio.gather(
                *[
                    self.vector_operations.create_semantic_vector(content)
                    for content in memory_contents
                ]
            )

            # Prepare the vectors for upsertion
            upsert_data = [
                {
                    "id": memory_id,
                    "values": vector,  # Updated to remove .tolist()
                    "metadata": metadata,
                }
                for memory_id, vector, metadata in zip(
                    memory_ids, semantic_vectors, memory_metadatas
                )
            ]

            # Upsert the batch of memories into Pinecone
            await self.pinecone_service.upsert_batch_memories(upsert_data)

            logger.info(f"Batch upserted {len(memories)} memories to Pinecone.")

        except Exception as e:
            logger.error(f"Error in batch embedding and storing memories: {e}")
            raise RuntimeError(f"Error in batch embedding and storing memories: {e}")

    async def generate_prompt(
        self, query: str, template_type: str, metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Retrieves memories, formats them into a prompt template, and returns the full prompt.

        Args:
            query: The user's query.
            template_type: Type of prompt template (e.g., 'RESEARCH_CONTEXT_TEMPLATE').
            metadata: Additional metadata required by the template.

        Returns:
            A formatted prompt.
        """
        query_vector = await self.vector_operations.create_semantic_vector(query)

        # Query both episodic and semantic memories
        episodic_memories = await self.query_memory(
            query_vector=query_vector, query_types=[MemoryType.EPISODIC], top_k=5
        )
        semantic_memories = await self.query_memory(
            query_vector=query_vector, query_types=[MemoryType.SEMANTIC], top_k=5
        )

        # Combine and rank the memories
        all_memories = episodic_memories + semantic_memories
        ranked_memories = self.rank_and_filter_results(all_memories, top_k=10)

        # Format the memories for the prompt template
        retrieved_context = "\n".join(
            [f"- {memory.content}" for memory, _ in ranked_memories]
        )

        # Select the appropriate template
        template = {
            "research_context": RESEARCH_CONTEXT_TEMPLATE,
            "personal_assistant": PERSONAL_ASSISTANT_TEMPLATE,
        }.get(template_type)

        if not template:
            raise ValueError(f"Unknown template type: {template_type}")

        # Format the prompt
        prompt = format_prompt(
            template,
            retrieved_context=retrieved_context,
            user_query=query,
            **(metadata or {}),
        )

        return prompt

    def build_metadata_filter(
        self, query_types: List[MemoryType], window_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Builds a metadata filter for Pinecone queries based on memory type and optionally window ID.

        Args:
            query_types: A list of MemoryTypes to include in the query.
            window_id: Optional window ID to filter episodic memories.

        Returns:
            A dictionary representing the metadata filter for Pinecone.
        """
        filter_dict = {}

        # Memory type filter
        if query_types:
            filter_dict["memory_type"] = {"$in": [mt.value for mt in query_types]}

        # Window ID filter (for episodic memories)
        if window_id:
            filter_dict["window_id"] = window_id

        return filter_dict

    def rank_and_filter_results(
        self, results: List[Tuple[Memory, float]], top_k: int
    ) -> List[Tuple[Memory, float]]:
        """Ranks and filters memory query results."""
        return sorted(results, key=lambda x: x[1], reverse=True)[:top_k]

    async def _get_old_memories(self, days: int) -> List[Memory]:
        """
        Retrieves memories older than a specified number of days.

        Args:
            days: The number of days to look back.

        Returns:
            A list of Memory objects that are older than the specified number of days.
        """
        cutoff_time = datetime.now() - timedelta(days=days)
        try:
            all_memories = await self.pinecone_service.get_all_memories_with_metadata()
            old_memories = [
                self._create_memory_from_result(mem)
                for mem in all_memories
                if "created_at" in mem["metadata"]
                and datetime.fromisoformat(mem["metadata"]["created_at"]) < cutoff_time
            ]
            return old_memories
        except Exception as e:
            logger.error(f"Error fetching old memories: {e}")
            return []

    def merge_metadata(self, memories: List[Memory]) -> Dict:
        """Merges metadata from multiple memories (not used for now)."""
        merged_metadata = {"content": " ".join([mem.content for mem in memories])}
        for memory in memories:
            for key, value in memory.metadata.items():
                if key in merged_metadata:
                    merged_metadata[key] += (
                        "; " + value if isinstance(value, str) else ""
                    )
                else:
                    merged_metadata[key] = value
        return merged_metadata

    def _create_memory_from_result(self, result: Dict[str, Any]) -> Memory:
        """Creates a Memory object from a Pinecone query result."""
        metadata = result.get("metadata", {})
        memory_type_str = metadata.get("memory_type")

        # Fallback to a default MemoryType if the value is not recognized
        try:
            memory_type = MemoryType(memory_type_str)
        except ValueError:
            logger.warning(
                f"Unrecognized memory type '{memory_type_str}'. Using default type."
            )
            memory_type = MemoryType.EPISODIC  # Or another default value

        return Memory(
            id=result.get("id", ""),
            content=metadata.get("content", ""),
            created_at=metadata.get("created_at", datetime.now().isoformat()),
            memory_type=memory_type,
            semantic_vector=result.get("values", []),
            metadata=metadata,
        )

    async def get_memories_by_window_id(self, window_id: str) -> List[Memory]:
        """Retrieves episodic memories associated with a specific window ID."""
        try:
            # Query Pinecone using the window_id in the metadata filter
            episodic_results = await self.query_memory(
                query_vector=np.zeros(1536).tolist(),  # Dummy vector, not used in filtering
                query_types=[MemoryType.EPISODIC],  # Filter for episodic memories
                top_k=100,
                window_id=window_id,
            )
            # Convert results to Memory objects
            memories = [memory for memory, _ in episodic_results]
            return memories

        except Exception as e:
            logger.error(f"Error getting memories by window ID: {e}")
            return []